from flask import Flask, request, jsonify
from transformers import pipeline
from flask_cors import CORS, cross_origin

app = Flask(__name__)
# CORS(app, support_credentials=True)
CORS(app)
# CORS(app)
# CORS(app, origins='http://localhost:5173', methods=['POST'], headers=['Content-Type'])

# Dummy transcript data
transcript = """ Any data science project starts with data collection process. At Lake Agriculture has three options of collecting data. First, we can use a ready-made data. We can either buy it from third-party vendor or get it from KGLE, etc. Second option is we can have a team of data annotators whose job is to collect these images from farmers and annotate those images either as a healthy or a late-blyde disease. This team of annotators can work with farmers. They can maybe go to the fields and ask farmers to take the pictures or they can take pictures themselves and classify with the help of farmers. Or by some means, by domain knowledge, these are classified as disease-ported plants versus the healthy-ported plants. So they can manually collect the data. This option is expensive. It requires budget. So you have to work with your stakeholders and kind of get the budget approved. And it might be time-consuming as well. The third option is data scientists can write web-scraping scripts to go through different websites which has ported images and collect those images and then use the tools like DOKANO. There are so many tools that are available which can help you annotate the data. So either you annotate that or you get annotated images by using those web-scraping tools. In this project, we are going to use redimate data from KGLE. We will be using this KGLE data set for our model training. You can click on this download button. It's 32629 megawatt beta whatever. And it has not only the images for ported disease classification but some tomato and pepper disease classification as well. We are going to ignore all of this. We will just focus on these three directories. So I had already previously downloaded this zip file. When I right-click and extract all I get this folder. And in this folder, I had the tomato, all these directories. But those directories I have deleted. So I deleted those directories manually. So I asked you to do the same thing. Go here, delete all the directories except these three. Then you will copy this directory into your project directory. Now for project directory, I have C code folder. And here I am going to create a new folder called ported to disease. So I want all of you to practice this code along with me. If you just watch my video, it's a waste of your time. You practice as you watch this video only then it is useful. You know, this is the best advice that someone can give you. Okay, I have this folder ready for my project. And in that, I am going to create a new folder called training. And then I am going to launch get bash. So I have this get bash which allows me to run all the Unix commands. You can use Windows command prompt as well. And I will run python minus am not book which is going to launch, you know, my Jupyter notebook here. And in this, I will locate my ported to disease folder, go training, create a new python three file. And this will be my model. Okay, so you can say, okay, training, whatever, just give some name to this particular notebook. And then we are going to import some essential library. So the purpose of this video actually is to download the data set into TF data set TF data input pipeline. And then we will do some data cleaning and we will make our data set ready for model training. So that's the purpose of this video. So here, let me download some essential, you know, modules here. And then the first thing I'm going to do is, okay, so we had this. Okay, so in the download my download folder somewhere, I had this plan village directory, right? So plan village directory, I'm going to do control C and then control V here. So I will copy all those images into the same folder where I'm running this notebook, you know, my ip and be not booked. So you see, now I have this directory. If you look at all this, this is like early blind. So there are 1000 images here. And if you look at all these images, you see there is there are this black dots. So this, this is showing that this prototype plant has some kind of disease. If you look at healthy plants, healthy leaves are healthy. You know, there are no blacks plots and they look pretty good. The other one, late blight will also have late blight is a little more deteriorating. See, you look at all these leaves, they look pretty horrible. So we have all this data here in our directory. And now I'm going to use tensor flows data set to download these images into TF dot data or data set. Now, if you don't know about TF data set, you need to pause this video right now. Go to YouTube search for tensor flow data input pipeline and you will see my video here. You need to watch this video. It will clarify your concepts. Basically, what's the purpose of TF dot data or data set? Let's say you have all these images on your hard disk. Okay. And you can download these images into batches because there could be so many images. So if you read these images and in batches into this TF data set structure, then you can do like dot filter dot map. You can do amazing things. So please watch this video and I will now assume that your concepts around TF data sets are clear. And we can now load that data using TF dot like this, this particular API. So TF dot kera dot pre process image data set from directory. Okay. Now, okay, what does this do? So you can search tensor flow image data set from directory. It will show you an API for this directory. So you specify directory first. So let's say you have main directory. You have your classes and you these are all the images. So this one call will load all the images into your tensor basically into your data set. Okay. So our so the first argument is what directory. Okay. What is our directory? Okay. So let me write this here. Our directory name is plant village. Correct. See plant village. That's our data directory. Then I will say shuffle is equal to true so that it will just randomly shuffle the images and load them. And then I will say image size. Okay. What is my image size? So let me go here and open this directory. You know, like if you look at this image size, you see 256 by 256. All of these images are 256 by 256. You can verify that. So I will say 256 by 256. But I will store. You know, I will create couple of constants where because I need to refer to this constants later. So I will say, okay, 256 by 256 is my image size. My batch size, you know, 32 is kind of like a standard batch size. I will again store that into a constant and initialize it here. And that's pretty much it. I will just say stole this into a data set. Okay. Okay. Okay. Okay. Okay. Okay. Okay. That's wrong. Okay. I did not run this. Okay. So it loaded to one five to five belonging to three classes. Well, which three classes. So you can just do this dot class names. You know, I was just stored that into a variable so that I can refer to it later. And these are the class names. Basically, your folder names are your class names. See, these are the three folder names. And if you look at this, this has 1000 images. The second one has 152. Card has 1000. So, 2001, 52. And look, if I do length of data set. Do you have any clue why is it showing 68? Just think about post the video thing about it. Because every element in the data set is actually a batch of 32 images. So if you do 60 to eight into 32. See, you the last batch is not perfect. So it is showing little more than 215 to images. But you got an idea. Why this is 68? Okay. Let's just explore. You know, I would say let's just explore this data set. So I will say for images batch. For image batch, label batch in data set dot take. You know, when you do this, it gives you one batch. One batch is how many images, 32 images. Okay. So I will print just the shape of this thing. I will say shape this and labels batch dot. I will just do see numpy like every element that you get is a tensor. So you need to convert that to numpy again. If you don't know the concept or refer to this video that I talked about earlier. And you find that there are 32 images. Each image is 256 by 256. Do you know what is this? You guys are smart. It's RGB. It's channels. Basically, you have RGB channels. So it's basically three channels. And I'm going to initialize that as well here. So that you know, I can refer to it little later. And these images label batch has you already realize 012. So this is 0. This is 1. This is 2. So there are 3 classes, 3 images. And if you want to print let's say each individual image. So I will forget about this. I will just print first image. This has 32 images. I will print first image. So for first image you see it's a tensor. If you want to convert tensor to a numpy, you do this. And you find all these numbers 3D array. Every number is between 0 to 255. The color is represented with 0 to 255. So that's what this is. And again, if you do shape of this, you'll find 256 by 256 by 3. First image got it. All right. Now let's try to visualize these images. Let's say I want to visualize this image. So I can use plt.imShow. So this is matplotlib. And when you do I am show, it expects 3D array. So what is my 3D array? Well, my 3D array is this. So I'm printing by the way the first image. So numpy. Okay. There is some problem. So what I need to do is it is float. So I converted it to int. And now you see it working. Okay. I don't care about all these numbers. So I will just do. I will just you know, hide that. And by the way, every time it is shuffling. So that's why every time you're seeing different image. It has shuffling randomness to it. X is off. Now I want to display the label. Like what image is that? So how do I display that label? Well, you can do plt.title. Okay. And what is my title? Well, my title is label batch. Okay. This is my title. But this will give you number 012. How can you get the exit this class name? Well, we have class name. So you supply that as an index. I hope you are getting the point. See, put it all the blight. Okay. I want to display couple of these images. So I will just run a full loop. I will say maybe I want to display out of, you know, first batch is 32. I want to display a less set to alley. And instead of this, I will say I, I got it. Okay. I hope that is clear. And if you want to show this enough, see if you run this, it's going to, it's just showing one. Why? Because you need to make a subplot. So subplot 3 by 4 is like almost like a metrics. And if you do this. Okay. It shows all the images, but the dimension is kind of messed up. So I will just increase the area, you know, of each of these images to 10 by 10. And look wonderful. It just shows me all the images beautifully. This is healthy leaf. See, this is only blight, late blight and so on. Now we are going to split our data set into train. They split. Okay. So let's say data set length is 68 actual length is by the way, 68 into 32 because each element is 32 batch. Okay. Now what we will do is we will keep 80% data as training data. Then we get remaining 20% right in remaining 20%. We will do two split. So one 10% split, we will do validation and remaining 10% will do test. So this validation set will be used during the training process. When you run each epoch, after each epoch, you do validation on this 10%. Okay. So you run let's say, you know, let me define the epoch. So I am going to run 50 epochs. This is trial and error. Okay. It could be 20, 30. So we will run less than 50 epochs and at the end of every epoch, we use this validation data set to do the validation. Once we are done through 50 epochs, once we have final model, then we use this 10% data set. This is called test data set to measure the accuracy of our model. Before we deploy our model into the wild, we want to use this 10% before we deploy our model into the wild, we'll use this 10% test data set to test the performance of our model. Now how do you get this split? You know in ask a learn, we have trained test split method. If you use statistical machine learning in ask a learn, we have that. We don't have that in dance floor. We are going to use data set dot take. So when you do data set dot take, okay, let's say 10, it will take first 10 samples. Now, what is our train size? Okay. So training size is 0.8 because it is 80% okay. And what is the length of our data set? 68. Okay. I'm going to say, okay, what is 80% of 68? Well, 54. So I can now do take first 54 samples. First 54 batches actually, each batch is 32. So it's much more simple and call it a train data set. Okay. So that's my train data set. And if you do a length, I hope you're practicing along with me. You find 54. And if you do data set dot skip 54, it means you are skipping first 54. And you're getting remaining 54. You know, this is like if you have used the slicing operator in an in Python list, it is like 54 onwards onwards. And this one is like first 54. Okay. So I hope if you know Python little bit, this should be clear. And this one. Okay. So first data. Okay. So this will be my test data set. Actually, this is not test data set. So this will give you remaining 10, 20% in that you need to against split into validation and test. Correct. So I mean, temporarily, I will save it as a test data set. But if this is not a utility data set, I have 14 and out of that. You know, my validation size is what 10% okay. And what I'm doing is 10% of my actual data set is 6. So I need six samples basically from my test data set. And when I do that, I get my validation data set. And if you do validation data set, that is six samples. And then you will do skip. And that will be your actual test data set. So we just split our data set into validation, taste and train data set. Now the code I wrote was using all the hard code numbers. And you know, it doesn't, it's just a prototype. So you want to wrap all of this into a nice looking Python function. Let's call it this function. And that function, the goal of this function is to take the TensorFlow data set. Okay, it should also take what is your split ratio. So I'm just saying if you don't supply anything by default, it will say 80% train, 10% validation, 10% test. And I'm also going to do shuffle. I'll explain why and suffice is 10,000. If you don't know about suffice again, watch to my other video that I referred. It's very important. You watch that. Okay. Now what I will return in the end is this. So we are doing whatever code we are doing. We are just creating a nice looking Python function. That's it. Okay. So my train size is. Okay. What is my data set size? First of all, so my data set size is this length of data set. Then my train size, my training size is train split like 80% of this. And I want to convert it into integer because see, I don't want to get these float numbers. That's my train size. And my validation size is this. Okay. All right. Now my train data set is basically whatever we did previously, which is. You know, DS dot take train size. And then when you do DS dot skip train size, you get a remaining 20% samples. In that, you will again take validation size. And that's where you get your valid data set. And if you do the same thing and just do skip here, you get your. Taste that asset. Okay. So I hope that is clear. Now we have shuffle arguments. So if shuffle, I want to just shuffle the data set. You know, so that before we split into train, test split, the. Suffoling happens. And seed is just for predictability. You know, if you do same seed every time, it will give you same result. This is just a seed number. It can be anything. It can be 5, 7, anything. Okay. My function is ready. And I can now call my function. On my data set. Okay. Here is the name of my data set. Here is my data set. You see data set. So we read all the images into this data set. Now we are doing train test. Right. Train test plate. Sorry. Okay. See this ran like. It ran so fast. And I will just confirm the size of my validation. Validation. Validation. Sad. My test said. And so on. And they are coming to be what we expect it to be actually. Now once again, if you have seen my video on TensorFlow data input pipeline, you would have understood the concepts behind caching and prefatching, etc. So that's what we are going to do here. So we are the training data set that we have. We will first do caching. This will, you know, it will read the image from the disk. And then for the next iteration, when you need the same image, it will keep that image in the memory. So this improves the performance of your pipeline. Again, watch that video because you will get good understanding on this shuffle. Okay. How shuffle thousand works again. You need to watch that video. So shuffle thousand will again like shuffle the images. I think this since our. Yeah, it can be less than 1000 as well. But anyways, and then pre-fetch, you know, pre-fetch if you are using GPU and CPU, if GPU is busy training, pre-fetch will load the next set of batch from your disk. And that will improve the performance. Actually, if you look at my deep learning playlist, I have pre-fetch and cache video here. So, you know, this video talks about pre-fetch and cache and I can quickly show you. So usually when you are loading batches, you know, there's 32 images at a time. And I have a GPU tight on RTX. When it is training, you know, you are not reusing CPU when the GPU is training because CPU is sitting idle. Then when you are done, now CPU again reads the batch and GPU's idle. So this, let's say for this example, it takes around 12 seconds. But if you use pre-fetch and caching, so what's going to happen is, see, when you use pre-fetch and caching, while GPU's training batch one, CPU will be loading that batch. You see? So that's your pre-fetch basically. And your cache is something where, okay, so this is pre-fetch and cache is basically, if you have read an image, so see here, I think, usually you see, you read an image. So this is that blue dot. And during the second epoch, you are reading the same images again, okay? But if you use cache, here, you don't see this blue block here. So you save time reading those images. So I will link all these videos, by the way. But if you do code basics, deep learning tutorials, you know, these are the two videos I am referring to. So back to the tutorial once again. So that's what I'm doing. And here, I'm letting TensorFlow determine how many batches to load, you know, while GPU is training. And then you can load this here. Okay. Now my validation and test dataset again will use the same paradigm. Now my these datasets are kind of optimized for training performance. So my training will run fast. Now we need to do some pre-processing. You all know if you have worked on any image processing, you know, the first thing we do is scale. So the image, the numpy array that we saw previously was between zero to 255, you know, it's an RGB scale. You want to divide that by 255 so that you get a number between zero and one. And the way you do that is by doing tf dot keras dot sequential. Okay. And here I'm supplying my pre-processing pipeline. Okay. So the way you do rescaling is by using this API. Now don't worry about experimental. By the way, this is stable. Okay. So we're just going to do a little bit of the validation with TensorFlow folks actually on this. They're saying it is stable. So don't worry 1.0. Dear by 255, this will just scale the image to 255. And we will supply this layer when we actually build our model. Okay. We need to do one more thing, which is resizing. We'll resize every image to 256 by 256. So this will resize the image. Now you will immediately ask me our images are already 256 by 256. Why do we need to resize it? But this layer that we are creating. Okay. See, let me create this layer. So this resize and rescale layer will eventually go to our ultimate model. And when we have a train model and when it starts predicting during prediction, if you are supplying any image, which is not 256 by 256, you know, some different dimension. This will take care of resizing it. So that's essentially the idea here. Once we have created this layer, one more thing we are going to do in terms of pre-processing is use data augmentation to, you know, make our model robust. Let's say you train a model using some images and then when you try predicting, you know, at that time, if you are supplying an image, which is rotated or which is not, which is different in a contrast, then your model will not perform better. So for that, we use a concept of data augmentation. In YouTube, you search for TensorFlow data augmentation. You'll find my, this my video, you must watch that video. What we do in that is let's say you have this kind of original image in your training data set. You create four new samples, four new training samples out of that. You apply different transformation. Let's say horizontal flip contrast. You see contrast is increased in this image. So you're taking same image, you're applying some filters, some contrast, some transformation. You are generating new training samples. See here, I rotated the images, you see. And I will use now all five images for my training. So I have one image, I create four extra image, I use all five images for my training. So that my model is robust tomorrow. When I start predicting in wild, if someone gives me rotated image, my model knows how to predict that. So that's the idea behind data augmentation. And if you have seen that video, TensorFlow provides beautiful APIs again, you're doing same thing where you're creating couple of layers. And I'm going to apply random flip and some rotation. If you watch that video, the other video, you will get a clear understanding. So that's my data augmentation layer, which I am going to store here. And by the way, resize, rescale all these layers, I'm going to use ultimately in my actual model. I had only this much for this video. In the next video, we are going to build a model and train it in this video. Just to summarize, we loaded our data into TensorFlow data set. We did some visualization. Then we did trained as split. And then we did some pre processing. We are not completed pre processing. We just created layers for pre processing, by the way. We will use these layers into our actual model. I hope you're liking it. I hope you're excited to see the next video coming where we'll be actually training the model. It's going to be a lot of fun. If you're liking this series, please share it with your friends. Give it a thumbs up. See, when you give it a thumbs up, it helps me with YouTube ranking. And this project can go to more people who are trying to learn. And the thing about YouTube is, you know, the learning is free. So if you are doing free learning, at least you can give it a thumbs up, you know, I mean, give it a thumbs down if you don't like it. I don't mind it. But if you give it a thumbs down, please leave a comment so that I can improve. Thank you for watching."""

# Initialize the question answering pipeline with a pre-trained model
qa_pipeline = pipeline("question-answering", model="distilbert-base-cased-distilled-squad", tokenizer="distilbert-base-cased")

# This function processes the user's doubt/query and generates a response
def process_doubt(user_doubt):
    # Use the question answering pipeline to find the answer in the transcript
    answer = qa_pipeline(question=user_doubt, context=transcript)

    return answer['answer']

# This route handles incoming doubt/query and responds based on the transcript
@app.route('/answer_doubt', methods=['POST'])
def answer_doubt():
    # Get the doubt/query from the request
    user_doubt = request.json.get('doubt')

    # Process the user's doubt/query
    response = process_doubt(user_doubt)

    return jsonify({"answer": response})

@app.route('/video_info', methods=['POST'])
def video_info():
    video_link = request.json.get('video_link')
 
    
    try:
        yt = YouTube(video_link)
        title = yt.title
        description = yt.description
        duration = yt.length
        response = {
            'title': title,
            'description': description,
            'duration': duration
        }
        return jsonify(response), 200
    except Exception as e:
        return jsonify({'error': str(e)}), 400

if __name__ == '__main__':
    app.run(debug=True)
